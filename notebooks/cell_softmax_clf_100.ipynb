{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Large-scale softmax classification for the drug screen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T01:54:57.428925Z",
     "start_time": "2021-05-13T01:54:52.288398Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext blackcellmagic\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import anndata as ad\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.optim as optim \n",
    "import torchvision.transforms as transforms \n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "\n",
    "import holoviews as hv \n",
    "from holoviews.operation.datashader import datashade, rasterize\n",
    "from holoviews.operation import gridmatrix\n",
    "import hvplot.pandas\n",
    "import colorcet as cc\n",
    "hv.extension('bokeh')\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "seed = 6398629\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T01:54:57.523517Z",
     "start_time": "2021-05-13T01:54:57.448517Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T01:55:02.543672Z",
     "start_time": "2021-05-13T01:54:57.527636Z"
    }
   },
   "outputs": [],
   "source": [
    "from magma import models as mm\n",
    "from magma import utils as mu\n",
    "mu.set_plotting_style_plt()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In brief, in this notebook we will build a cell classifier model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T01:55:02.641290Z",
     "start_time": "2021-05-13T01:55:02.549836Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "url = 'https://github.com/manuflores/sandbox/blob/master/figs/cell_clf.png?raw=true'\n",
    "\n",
    "Image(\n",
    "    url = url, format = 'png', width = 500, height = 500\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Digression on using other models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we talked during the meeting, we are using this classifier model as a pre-training step in order to have a good vectorized representation of the cells amenable to use the contrastive learning procedure. \n",
    "\n",
    "We could in theory use other variables of interest. For example, we can train a model to predict the drug target or the pathway of the molecule that perturbed the cell. We have information about metadata about the drugs. \n",
    "\n",
    "We could also use other models like Variational Autoencoder or any other type of encoder model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from the diagram above, the input for this model will be a cell (transcriptome) and the output will be the label of the drug that perturbed it. \n",
    "\n",
    "We're going to use a subset of our drug screen dataset that contains data of only 100 drugs, from **myeloid cells subjected to the CD3 treatment**. We could in fact use the data from all cell types, but this is just a proof-of-concept. . The idea is that we're would later use the data from the other samples for validation for the joint embedding. \n",
    "\n",
    "For context, the data is in log-normalized, i.e. normalized by the total number of counts per cell and then scaled by a factor of $10^4$, to be finally set in log scale. In other words we used the following equation on the counts of each gene: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\tilde{g_{i}} = \\mathrm{ln} \\left( \\frac{g_i \\times 10^4}{\\sum_{i}^n g_i} + 1 \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\tilde{g_i}$ is the log-normalized counts for gene $i$ for a given cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The single-cell data count matrix is in [`anndata`](https://anndata.readthedocs.io/) format. This is the python analog of the Seurat object from R. [I highly recommend this tutorial](https://falexwolf.de/blog/171223_AnnData_indexing_views_HDF5-backing/) from Alex Wolf, one of the main developers of AnnData, that introduces how to use numpy-like indexing (numerical indexing) and pandas-like (indexing with boolean Series meeting categorical criteria). I have another tutorial working with single-cell RNA seq data [here](https://github.com/manuflores/sandbox/blob/master/notebooks/qc_cv_filering.ipynb) if you're interested, but the basic idea is that you can use indexing both in the rows and columns of the `anndata`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load cell dataset: CD3 (+) myeloid cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the data and take a look. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T01:55:02.940796Z",
     "start_time": "2021-05-13T01:55:02.644899Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import dataset\n",
    "a = ad.read_h5ad('mult_cd3_100_train.h5ad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T01:55:03.041651Z",
     "start_time": "2021-05-13T01:55:02.944873Z"
    }
   },
   "outputs": [],
   "source": [
    "# Take a look\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T01:55:03.139784Z",
     "start_time": "2021-05-13T01:55:03.045068Z"
    }
   },
   "outputs": [],
   "source": [
    "type(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T01:55:03.236674Z",
     "start_time": "2021-05-13T01:55:03.144809Z"
    }
   },
   "outputs": [],
   "source": [
    "type(a.obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T01:55:03.340327Z",
     "start_time": "2021-05-13T01:55:03.240735Z"
    }
   },
   "outputs": [],
   "source": [
    "type(a.var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T01:55:03.456628Z",
     "start_time": "2021-05-13T01:55:03.343071Z"
    }
   },
   "outputs": [],
   "source": [
    "# Contains metadata from the cells \n",
    "a.obs.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T01:55:03.559224Z",
     "start_time": "2021-05-13T01:55:03.460237Z"
    }
   },
   "outputs": [],
   "source": [
    "# Contains metadata from the genes. \n",
    "a.var.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T01:55:03.652796Z",
     "start_time": "2021-05-13T01:55:03.562956Z"
    }
   },
   "outputs": [],
   "source": [
    "print('We have data from cells subjected to %d perturbations.'%a.obs.drug_name.unique().shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very briefly, I'll assume you have no experience with anndata and filter both rows and columns. \n",
    "\n",
    "1. Filter to get only nilotinib cells (rows).\n",
    "2. Filter to get only data from the first 50 cells and genes with [coefficient of variation (CV)](https://en.wikipedia.org/wiki/Coefficient_of_variation) higher than 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T01:55:03.756943Z",
     "start_time": "2021-05-13T01:55:03.656201Z"
    }
   },
   "outputs": [],
   "source": [
    "# 1. notice how we get only 700 cells\n",
    "a[a.obs.drug_name == 'nilotinib']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T01:55:03.858077Z",
     "start_time": "2021-05-13T01:55:03.760164Z"
    }
   },
   "outputs": [],
   "source": [
    "# 2 - notice how we get only 50 cells and 1045 genes\n",
    "a[:50, a.var.cv > 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All right let's proceed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T01:55:03.957863Z",
     "start_time": "2021-05-13T01:55:03.861745Z"
    }
   },
   "outputs": [],
   "source": [
    "# Check that we only have myeloid cells\n",
    "a.obs.cell_type.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T01:55:04.063992Z",
     "start_time": "2021-05-13T01:55:03.967058Z"
    }
   },
   "outputs": [],
   "source": [
    "# Check that we only have CD3+ data\n",
    "a.obs.CD3.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly to the molecule classification case, we need to make sure that the data is balanced. Let's check the samples with the most and the lowest number of cells. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T01:55:04.163330Z",
     "start_time": "2021-05-13T01:55:04.071249Z"
    }
   },
   "outputs": [],
   "source": [
    "# Samples with most cells\n",
    "a.obs.sample_id.value_counts().head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T01:55:04.258496Z",
     "start_time": "2021-05-13T01:55:04.166623Z"
    }
   },
   "outputs": [],
   "source": [
    "# Samples with least number of cells\n",
    "a.obs.sample_id.value_counts().tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T01:55:04.355735Z",
     "start_time": "2021-05-13T01:55:04.261339Z"
    }
   },
   "outputs": [],
   "source": [
    "# Remove Nilotinib, Ketoprofen and Eupatilin for out-of-sample predictions\n",
    "#a = a[~a.obs.drug_name.isin(['Nilotinib', 'Ketoprofen', 'Eupatilin'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put a cap on using at most 500 cells and sample ! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T01:55:06.808618Z",
     "start_time": "2021-05-13T01:55:04.358810Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "n_samples = 500\n",
    "\n",
    "sampling_ix = (\n",
    "    a.obs.groupby([\"sample_id\"])\n",
    "    .apply(\n",
    "        lambda group_df: group_df.sample(\n",
    "            group_df.shape[0] if group_df.shape[0] < n_samples else n_samples,\n",
    "            replace = False)\n",
    "    )\n",
    "    .index.get_level_values(1) # Get the numerical index :) \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use this index to select the data numpy-style."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T01:55:07.078982Z",
     "start_time": "2021-05-13T01:55:06.812326Z"
    }
   },
   "outputs": [],
   "source": [
    "# Select cells\n",
    "ada = a[sampling_ix].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's confirm that we indeed selected the data correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T01:55:07.220645Z",
     "start_time": "2021-05-13T01:55:07.082488Z"
    }
   },
   "outputs": [],
   "source": [
    "ada.obs.sample_id.value_counts().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T01:55:07.320606Z",
     "start_time": "2021-05-13T01:55:07.225390Z"
    }
   },
   "outputs": [],
   "source": [
    "ada.obs.sample_id.value_counts().tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent let's proceed to make categorical indices of the samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T01:55:07.419082Z",
     "start_time": "2021-05-13T01:55:07.323563Z"
    }
   },
   "outputs": [],
   "source": [
    "codes, uniques = pd.factorize(ada.obs[\"sample_id\"].values.to_list())\n",
    "ada.obs['sample_codes'] = codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can make our train-test split using stratified sampling, keeping the same proportion of the drug perturbations and the drug target in each fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T01:55:07.509393Z",
     "start_time": "2021-05-13T01:55:07.422657Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T01:55:07.725938Z",
     "start_time": "2021-05-13T01:55:07.512382Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize stratified splitter\n",
    "splitter = StratifiedShuffleSplit(n_splits = 1, test_size = 0.4, random_state = seed)\n",
    "\n",
    "# Get indices \n",
    "ixs = list(splitter.split(ada.X, ada.obs[['sample_codes', 'target']]))\n",
    "\n",
    "# Extract train and test indices\n",
    "train_ixs, val_ixs = ixs[0][0], ixs[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T01:55:08.005498Z",
     "start_time": "2021-05-13T01:55:07.729882Z"
    }
   },
   "outputs": [],
   "source": [
    "train_adata = ada[train_ixs].copy()\n",
    "test_adata = ada[val_ixs].copy()\n",
    "# train_adata = a[train_ixs].copy()\n",
    "# test_adata = a[val_ixs].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that we have the same proportion of "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T01:55:08.104092Z",
     "start_time": "2021-05-13T01:55:08.008773Z"
    }
   },
   "outputs": [],
   "source": [
    "train_adata.obs.sample_id.value_counts().tail()/ train_adata.n_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T01:55:08.205930Z",
     "start_time": "2021-05-13T01:55:08.107508Z"
    }
   },
   "outputs": [],
   "source": [
    "test_adata.obs.sample_id.value_counts().tail()/ test_adata.n_obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making torch datasets and dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the graph case, we also need to construct torch datasets and dataloaders. The most important I think is the torch dataset, because it is highly dependent on the type of data you're using (images, text, speech, or numerical vectors) and is almost always in the need of customization. I have written code to make torch datasets using anndata so we're good. The basic idea though is that we need to override the `torch.Dataset` base class with a  `__getitem__()` method, that allows to extract data by indexing (we'll show an example below). You can imagine that if you're using a supervised method one has to also write down a way to extract the label $y_i$ for each input data point $x_i$. This can be done in a highly efficient manner as to read data from disk and just feed it by minibatches through the `DataLoader` generator. So that's the reason that they exist, they allow you to feed data into a model using minibatches from disk, that could in theory not be handled if loaded in memory. I actually wrote a function to do this using numpy arrays instead of `anndataset`s for extremely large count matrices but I hope that this dataset would fit fine in memory as it is 0.1 GB in size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T01:55:08.304433Z",
     "start_time": "2021-05-13T01:55:08.208907Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize torch dataset \n",
    "train_dataset = mu.adata_torch_dataset(\n",
    "    train_adata,\n",
    "    transform = transforms.ToTensor(),\n",
    "    supervised = True,\n",
    "    target_col = 'sample_codes'\n",
    ")\n",
    "\n",
    "test_dataset = mu.adata_torch_dataset(\n",
    "    test_adata,\n",
    "    transform = transforms.ToTensor(),\n",
    "    supervised = True,\n",
    "    target_col = 'sample_codes',\n",
    "    #multilabel = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T01:55:08.413127Z",
     "start_time": "2021-05-13T01:55:08.307869Z"
    }
   },
   "outputs": [],
   "source": [
    "# Extract the first datapoint from the training set:\n",
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we can index the train dataset object and it returns a tuple of a) a tensor containing the log-normed counts and b) the label for the drug perturbation. With this torch dataset in place, we can go ahead and initialize the dataloaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T01:55:08.516899Z",
     "start_time": "2021-05-13T01:55:08.422788Z"
    }
   },
   "outputs": [],
   "source": [
    "# check your processors, in case you don't know yet !\n",
    "import multiprocessing as mp\n",
    "n_cores = mp.cpu_count()\n",
    "\n",
    "print('We have %d available cores'%n_cores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T01:55:08.620117Z",
     "start_time": "2021-05-13T01:55:08.523722Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 64  # increase batch size because of large dataset\n",
    "\n",
    "# Initialize DataLoader for minibatching\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    drop_last=True,\n",
    "    shuffle=False,\n",
    "    num_workers=n_cores,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    drop_last=True,\n",
    "    shuffle=False,\n",
    "    num_workers=n_cores,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All right, we're ready to initialize our classifier. We will be using a simple deep MLP stored in the `mm.supervised_model()` class. We only need to specify the dimensionalities of the input, output and intermediate layers of the MLP. We will start with a dimensionality equal to the number of genes, and the final output dimension is the number of drugs to predict, in this case 100. Finally we will add three intermediate layers to enable the capacity of learning optimized representations of size 512, 256, and 64. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T01:55:08.718465Z",
     "start_time": "2021-05-13T01:55:08.623725Z"
    }
   },
   "outputs": [],
   "source": [
    "#help(mm.supervised_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T01:55:08.813710Z",
     "start_time": "2021-05-13T01:55:08.721808Z"
    }
   },
   "outputs": [],
   "source": [
    "number_of_drugs = len(uniques)\n",
    "number_of_genes = ada.n_vars\n",
    "\n",
    "# Initialize dimensionalities list\n",
    "dims = [number_of_genes, 512, 256, 64, number_of_drugs]\n",
    "\n",
    "dims"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can initialize the model, we will set the model type to `multiclass`, and we won't use dropout for the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T01:55:08.924453Z",
     "start_time": "2021-05-13T01:55:08.816546Z"
    }
   },
   "outputs": [],
   "source": [
    "model = mm.supervised_model(dims, model = 'multiclass', dropout = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's initialize the weights of our model (if you want a nice explanation of why this is important [check this lecture notes](https://www.deeplearning.ai/ai-notes/initialization/index.html)), and initialize our optimizer and error function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T01:55:09.037350Z",
     "start_time": "2021-05-13T01:55:08.928669Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize weights according to Xavier \n",
    "model = mu.initialize_network_weights(model, method = 'xavier_normal')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3, weight_decay = 0)\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T01:55:09.135007Z",
     "start_time": "2021-05-13T01:55:09.039984Z"
    }
   },
   "outputs": [],
   "source": [
    "# Take a look at the method. \n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It will take around 30mins - 1 hour to train the model. The trainer for MLPs currently has much more functionality than for GCNs. We can save models every epoch by supplying a valid directory in the `model_dir` kwarg and a corresponding `model_name`. We can also add a ratio for early stopping if we don't want to monitor the validation loss for convergence. The early stopping is activated when the validation loss increases a fraction of `early_stopping_tol` (by default 0.2) with respect to a previous epoch. If you want to let the model run substantially, set the `n_epochs` parameter to 200 and let the early stopping handle the rest. If for some reason you don't want early stopping, set the `early_stopping_tol` parameter to 50. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extra note for running on a GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We havent covered this but we could also run the model in a GPU. We do this under the hood with the trainer function but you can explicity do this by runnning the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T01:55:09.232701Z",
     "start_time": "2021-05-13T01:55:09.137962Z"
    }
   },
   "outputs": [],
   "source": [
    "# device = nm.try_gpu()\n",
    "# model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T01:55:09.326244Z",
     "start_time": "2021-05-13T01:55:09.235760Z"
    }
   },
   "outputs": [],
   "source": [
    "# You can set a directory to save the weights per each epoch.\n",
    "#model_dir = path + '../models/droog_100/'\n",
    "n_epochs = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T01:55:41.782448Z",
     "start_time": "2021-05-13T01:55:09.329549Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "train_loss, val_loss = mu.supervised_trainer(\n",
    "    n_epochs,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    model,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    multiclass = True,\n",
    "    n_classes = number_of_drugs, \n",
    "    model_dir = None, # for example './' to save in current directory\n",
    "    model_name = None, # for example 'classiffier_100_drugs'\n",
    "    early_stopping_tol = 0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a previous run, the model at 2 epochs was the best. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T01:55:41.882576Z",
     "start_time": "2021-05-13T01:55:41.786893Z"
    }
   },
   "outputs": [],
   "source": [
    "#model = nm.supervised_model(dims, model = 'multiclass', dropout = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T01:55:41.980766Z",
     "start_time": "2021-05-13T01:55:41.886858Z"
    }
   },
   "outputs": [],
   "source": [
    "#model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T01:55:42.080104Z",
     "start_time": "2021-05-13T01:55:41.985104Z"
    }
   },
   "outputs": [],
   "source": [
    "# trained_weights = torch.load(path + 'droog_clf_100.pt')\n",
    "# model.load_state_dict(trained_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check our classification accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T01:55:42.195193Z",
     "start_time": "2021-05-13T01:55:42.083923Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Make predictions in test set \n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    y_pred = mu.supervised_model_predict(\n",
    "        model,\n",
    "        val_loader,\n",
    "        criterion,\n",
    "        multiclass = True, \n",
    "        n_outputs = n_cats,\n",
    "        score = True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T01:55:42.306914Z",
     "start_time": "2021-05-13T01:55:42.198034Z"
    }
   },
   "outputs": [],
   "source": [
    "acc = (y_pred.argmax(axis = 1) == test_adata.obs.sample_codes.values).sum() / test_adata.n_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T01:55:42.322006Z",
     "start_time": "2021-05-13T01:54:52.408Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Accuracy of the model is %.3f'%acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the GCN example, we can compute the confusion matrix of the predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T01:55:42.324355Z",
     "start_time": "2021-05-13T01:54:52.410Z"
    }
   },
   "outputs": [],
   "source": [
    "conf_mat = mu.confusion_matrix(df_proj.y_pred, df_proj.sample_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T01:55:42.326884Z",
     "start_time": "2021-05-13T01:54:52.413Z"
    }
   },
   "outputs": [],
   "source": [
    "df_conf_mat = pd.DataFrame(\n",
    "    conf_mat, \n",
    "    columns = uniques,\n",
    "    index = uniques\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T01:55:42.329086Z",
     "start_time": "2021-05-13T01:54:52.416Z"
    }
   },
   "outputs": [],
   "source": [
    "df_conf_mat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project to latent space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's initialize a new data loader to get the cell low dimensional embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T01:55:42.330767Z",
     "start_time": "2021-05-13T01:54:52.418Z"
    }
   },
   "outputs": [],
   "source": [
    "projection_dataset = mu.adata_torch_dataset(\n",
    "    data = a,\n",
    "    transform = transforms.ToTensor(),\n",
    "    supervised = True,\n",
    "    target_col = 'sample_codes'\n",
    ")\n",
    "\n",
    "# Initialize DataLoader for projection\n",
    "projection_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " I didn't go through this in the last tutorial, but the signature call of the projection function returns a [generator](https://realpython.com/introduction-to-python-generators/) to make it a bit more efficient, that's why the model is wrapped under a list call, and then under a `np.array()` function to return the embeddings as a matrix of size `n_cells` by `n_dimensions`. In our case, the dimension of the last hidden layer was 64. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T01:55:42.332752Z",
     "start_time": "2021-05-13T01:54:52.421Z"
    }
   },
   "outputs": [],
   "source": [
    "# Returns a generator\n",
    "x = model.project_to_latent_space(\n",
    "    data_loader = projection_loader,\n",
    "    n_feats = n_genes, \n",
    "    latent_dim = dims[-2] # last hidden dimension (64)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T01:55:42.335363Z",
     "start_time": "2021-05-13T01:54:52.423Z"
    }
   },
   "outputs": [],
   "source": [
    "type(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T01:55:42.337686Z",
     "start_time": "2021-05-13T01:54:52.425Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get the first embedding\n",
    "embedding = next(iter(x))\n",
    "print('Size of the cell embeddings is %d.'%embedding.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get the actual embeddings this time and save them into a dataframe for further visualization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T01:55:42.339364Z",
     "start_time": "2021-05-13T01:54:52.427Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "\n",
    "    projection_arr = np.array(\n",
    "        list(model.project_to_latent_space(projection_loader, dims[0], dims[-2]))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T01:55:42.341279Z",
     "start_time": "2021-05-13T01:54:52.429Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    projection_arr, columns = ['latent_' + str(i) for i in range(1, dims[-2] + 1)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T01:55:42.343147Z",
     "start_time": "2021-05-13T01:54:52.432Z"
    }
   },
   "outputs": [],
   "source": [
    "df_proj = pd.concat(\n",
    "    [test_adata.obs, df.set_index(test_adata.obs.index)], axis=1\n",
    ")\n",
    "\n",
    "# Add model predictions to df\n",
    "df_proj['y_pred'] = y_pred.argmax(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T01:55:42.344973Z",
     "start_time": "2021-05-13T01:54:52.434Z"
    }
   },
   "outputs": [],
   "source": [
    "df_proj.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T01:55:42.347140Z",
     "start_time": "2021-05-13T01:54:52.436Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "\n",
    "%watermark -m -v -p numpy,torch,pandas,anndata,sklearn,scipy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:boltzmann] *",
   "language": "python",
   "name": "conda-env-boltzmann-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
